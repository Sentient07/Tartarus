{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import theano\n",
    "import theano.tensor as tensor\n",
    "import lasagne\n",
    "import lasagne.layers as ll\n",
    "from lasagne.nonlinearities import softmax, linear\n",
    "import lasagne.init as init\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell applies patch/kernel wise Spatial transformation in the feature space. It takes in two convolution layers of the same height and width. It then applies affine transformation, followed by bilinear interpolation with upsampling factor same as the stride. There are two loops to do this, after applying the Spatial Transformation, the layers are stacked to fill the rows and then stacked one below the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper Cell\n",
    "\n",
    "\n",
    "class AppendLayer(ll.Layer):\n",
    "    '''\n",
    "    This layer takes in a Lasagne Layer and constants which are the center coordinates of the Kernel and\n",
    "    affine transformation\n",
    "    \n",
    "    incoming : Layer\n",
    "    Input Lasagne Layer\n",
    "    \n",
    "    neurons : tuple\n",
    "    The center coordinates\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, incoming, neurons, **kwargs):\n",
    "        super(AppendLayer, self).__init__(incoming, neurons, **kwargs)\n",
    "        assert isinstance(neurons, tuple)\n",
    "        # Only Two parameters describing the center can be passed\n",
    "        assert len(neurons) == 2\n",
    "        self.neurons = neurons\n",
    "        self.incoming = incoming\n",
    "        inc_shape = incoming.output_shape\n",
    "        self.n_len = len(neurons)\n",
    "        self.out_shp = (inc_shape[0], inc_shape[1] + len(self.neurons))\n",
    "        \n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        out_tensor = tensor.zeros(self.out_shp)\n",
    "        n_len = self.n_len * -1\n",
    "        # The replacement is being done in 4 set_subtensor steps as \n",
    "        # the 3rd and 6th position corresponds to zoom\n",
    "        inp_replaced = tensor.set_subtensor(out_tensor[:, :2], input[:, :2])\n",
    "        inp_replaced = tensor.set_subtensor(inp_replaced[:, 3:5], input[:, 2:])\n",
    "        center_replaced = tensor.set_subtensor(inp_replaced[:, 3], self.neurons[0])\n",
    "        center_replaced = tensor.set_subtensor(center_replaced[:, -1], self.neurons[-1])\n",
    "        return center_replaced\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1] + len(self.neurons))\n",
    "\n",
    "\n",
    "def patchwise_st(incoming, l_loc, ss, w_t=init.Constant(0.01), b_t=None):\n",
    "    \n",
    "    '''\n",
    "    incoming : Lasagne Layer\n",
    "    The incoming Layer. It should be of the shape (2xBS, C, W, H)\n",
    "    \n",
    "    l_loc : Lasagne Layer\n",
    "    The feature map that would be used to compute the parameters of affine transformation.\n",
    "    \n",
    "    ss : int\n",
    "    Stride Size.\n",
    "\n",
    "    '''\n",
    "    listi = []\n",
    "    listj = []\n",
    "    inc_shape = incoming.output_shape\n",
    "    for j in range(0, inc_shape[-2] - ss, ss):\n",
    "        for i in range(0, inc_shape[-1] - ss, ss):\n",
    "            sliced_layer = ll.SliceLayer(ll.SliceLayer(incoming, indices=slice(j, j+ss), axis=2),\n",
    "                                         indices=slice(i, i+ss), axis=3)\n",
    "            # The Y and X center coordinates of the Kernel\n",
    "            # Here, it's assumed that kernel has odd dimension\n",
    "            v_l = (i + ss // 2 - 1, i + ss // 2)\n",
    "            h_l = (j + ss // 2 - 1, j + ss // 2)\n",
    "            \n",
    "            # Reshaping into (batch_size, 4) which are the affine transformation parameters\n",
    "            sliced_loc = ll.ReshapeLayer(ll.SliceLayer(ll.SliceLayer(l_loc, indices=slice(*v_l), axis=2),\n",
    "                                                       indices=slice(*h_l), axis=3), (-1, 4))\n",
    "            final_loc = AppendLayer(sliced_loc, (h_l[0], v_l[0]))\n",
    "            l_trans = ll.TransformerLayer(sliced_layer, final_loc, downsample_factor=np.float32(1.0/ss))\n",
    "\n",
    "            if not listi:\n",
    "                listi.append(l_trans)\n",
    "            else:\n",
    "                listi.append(ll.ConcatLayer([listi[-1], l_trans], axis=3))\n",
    "        if not listj:\n",
    "            listj.append(listi[-1])\n",
    "        else:\n",
    "            listj.append(ll.ConcatLayer([listj[-1], listi[-1]], axis=2))\n",
    "        listi = []\n",
    "    return listj[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells are for building the model. The Dense feature extractor is built similar to fc6-conv described here, https://github.com/BVLC/caffe/blob/master/examples/net_surgery/bvlc_caffenet_full_conv.prototxt#L156-L164. The output is resized to `(batch_size, C x H x W)` for ease of error computation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_stinception(name, input_layer, nfilters=[64, 192, 96, 32, 32, 4, 224]):\n",
    "    \n",
    "    '''\n",
    "    name : str\n",
    "    Name of this network\n",
    "    \n",
    "    input_layer : Lasagne Layer\n",
    "    Layer over which Inception has to be applied\n",
    "    \n",
    "    nfilters : List\n",
    "    List containing integers of filter depth\n",
    "    \n",
    "    '''\n",
    "    net = {}\n",
    "    net['pool'] = ll.MaxPool2DLayer(input_layer, pool_size=3, stride=1, pad=1)\n",
    "    net['pool_proj'] = ll.Conv2DLayer(net['pool'], nfilters[0], 1, flip_filters=False)\n",
    "\n",
    "    net['1x1'] = ll.Conv2DLayer(input_layer, nfilters[1], 1, flip_filters=False)\n",
    "\n",
    "    net['3x3_reduce'] = ll.Conv2DLayer(input_layer, nfilters[2], 1, pad=1, flip_filters=False)\n",
    "    net['3x3_reduce_param'] = ll.Conv2DLayer(net['3x3_reduce'], nfilters[3], 1, flip_filters=False)\n",
    "    net['3x3_reduce_param2'] = ll.Conv2DLayer(net['3x3_reduce_param'], nfilters[4], 1, flip_filters=False)\n",
    "    net['3x3_reduce_param3'] = ll.Conv2DLayer(net['3x3_reduce_param2'], nfilters[5], 1, flip_filters=False, \n",
    "                                              nonlinearity=linear, pad=0)\n",
    "    net['3x3_spp'] = patchwise_st(net['3x3_reduce'], net['3x3_reduce_param3'], 3)\n",
    "    net['3x3'] = ll.Conv2DLayer(net['3x3_spp'], nfilters[6], 3, stride=3, pad=5, flip_filters=False)\n",
    "\n",
    "    net['5x5_reduce'] = ll.Conv2DLayer(input_layer, nfilters[2], 1, pad=2, flip_filters=False)\n",
    "    net['5x5_reduce_param'] = ll.Conv2DLayer(net['5x5_reduce'], nfilters[3], 1, flip_filters=False)\n",
    "    net['5x5_reduce_param2'] = ll.Conv2DLayer(net['5x5_reduce_param'], nfilters[4], 1, flip_filters=False)\n",
    "    net['5x5_reduce_param3'] = ll.Conv2DLayer(net['5x5_reduce_param2'], nfilters[5], 1, flip_filters=False, \n",
    "                                              nonlinearity=linear, pad=0)\n",
    "    net['5x5_spp'] = patchwise_st(net['5x5_reduce'], net['5x5_reduce_param3'], 5)    \n",
    "    net['5x5'] = ll.Conv2DLayer(net['5x5_spp'], nfilters[6], 5, stride=5, pad=5, flip_filters=False)\n",
    "    net['output'] = ll.ConcatLayer([\n",
    "        net['1x1'],\n",
    "        net['3x3'],\n",
    "        net['5x5'],\n",
    "        net['pool_proj'],\n",
    "        ])\n",
    "    return {'{}/{}'.format(name, k): v for k, v in net.items()}\n",
    "\n",
    "\n",
    "# The Google LeNet's inception module has been used from Lasagne's modelzoo\n",
    "# https://github.com/Lasagne/Recipes/blob/master/modelzoo/googlenet.py#L22-L48\n",
    "\n",
    "def build_inception_module(name, input_layer, nfilters, conv_st=False):\n",
    "    # nfilters: (pool_proj, 1x1, 3x3_reduce, 3x3, 5x5_reduce, 5x5)\n",
    "    net = {}\n",
    "    net['pool'] = ll.MaxPool2DLayer(input_layer, pool_size=3, stride=1, pad=1)\n",
    "    net['pool_proj'] = ll.Conv2DLayer(net['pool'], nfilters[0], 1, flip_filters=False)\n",
    "\n",
    "    net['1x1'] = ll.Conv2DLayer(input_layer, nfilters[1], 1, flip_filters=False)\n",
    "\n",
    "    net['3x3_reduce'] = ll.Conv2DLayer(input_layer, nfilters[2], 1, flip_filters=False)\n",
    "    net['3x3'] = ll.Conv2DLayer(net['3x3_reduce'], nfilters[3], 3, pad=1, flip_filters=False)\n",
    "\n",
    "    net['5x5_reduce'] = ll.Conv2DLayer(input_layer, nfilters[4], 1, flip_filters=False)\n",
    "    net['5x5'] = ll.Conv2DLayer(net['5x5_reduce'], nfilters[5], 5, pad=2, flip_filters=False)\n",
    "\n",
    "    net['output'] = ll.ConcatLayer([\n",
    "        net['1x1'],\n",
    "        net['3x3'],\n",
    "        net['5x5'],\n",
    "        net['pool_proj'],\n",
    "        ])\n",
    "    return {'{}/{}'.format(name, k): v for k, v in net.items()}\n",
    "\n",
    "\n",
    "# The number of feature to be the output has been taken to 1024. According to Figure A1 in the paper,\n",
    "# the value has to be an user input. However, the number of output cannot be symbolic and hence I'm assuming it\n",
    "# to be 1024. \n",
    "\n",
    "def build_model(i_var, n_dense_out=1024):\n",
    "    net = {}\n",
    "    net['input'] = ll.InputLayer((10, 3, 256, 256), input_var=i_var)\n",
    "    net['conv1/7x7_s2'] = ll.Conv2DLayer(net['input'], 64, 7, stride=2, pad=3, flip_filters=False)\n",
    "    net['pool1/3x3_s2'] = ll.MaxPool2DLayer(\n",
    "        net['conv1/7x7_s2'], pool_size=3, stride=2, ignore_border=False)\n",
    "    net['pool1/norm1'] = ll.LocalResponseNormalization2DLayer(net['pool1/3x3_s2'], alpha=0.00002, k=1)\n",
    "    net['conv2/3x3_reduce'] = ll.Conv2DLayer(\n",
    "        net['pool1/norm1'], 64, 1, flip_filters=False)\n",
    "    net['conv2/3x3'] = ll.Conv2DLayer(\n",
    "        net['conv2/3x3_reduce'], 192, 3, pad=1, flip_filters=False)\n",
    "    net['conv2/norm2'] = ll.LocalResponseNormalization2DLayer(net['conv2/3x3'], alpha=0.00002, k=1)\n",
    "    net['pool2/3x3_s2'] = ll.MaxPool2DLayer(\n",
    "      net['conv2/norm2'], pool_size=3, stride=2, ignore_border=False)\n",
    "\n",
    "    net.update(build_inception_module('inception_3a',\n",
    "                                      net['pool2/3x3_s2'],\n",
    "                                      [32, 64, 96, 128, 16, 32]))\n",
    "    net.update(build_inception_module('inception_3b',\n",
    "                                      net['inception_3a/output'],\n",
    "                                      [64, 128, 128, 192, 32, 96]))\n",
    "    net['pool3/3x3_s2'] = ll.MaxPool2DLayer(\n",
    "      net['inception_3b/output'], pool_size=3, stride=2, ignore_border=False)\n",
    "\n",
    "    net.update(build_stinception('inception_4a', net['pool3/3x3_s2']))\n",
    "    net['feature_unnorm'] = ll.Conv2DLayer(net['inception_4a/output'], 128, 1)\n",
    "    net['feature'] = ll.Conv2DLayer(net['feature_unnorm'], n_dense_out, 6, pad=1, nonlinearity=linear)\n",
    "    net['out'] = ll.ReshapeLayer(net['feature'], (10, -1))\n",
    "    return net\n",
    "\n",
    "\n",
    "def nearest_neighbour(img1, img2, num_p):\n",
    "    '''\n",
    "    img1 : Theano Variable\n",
    "    Image 1\n",
    "    \n",
    "    img2 : Theano Variable\n",
    "    Image 2\n",
    "    \n",
    "    num_p : Theano Scalar (int32)\n",
    "    The number of nearest points desired\n",
    "    \n",
    "    Returns : \n",
    "    The magnitude of minimum tolerance. The points/pixels that are farther from original image in the\n",
    "    eucledian space are negative, and the points that are closer are potitive\n",
    "\n",
    "    '''\n",
    "    b_size = img1.shape[0]\n",
    "    n_feature1 = img1.shape[1]\n",
    "    n_feature2 = img2.shape[1]\n",
    "    xL2S = tensor.sum(tensor.sqr(img1), axis=-1)\n",
    "    yL2S = tensor.sum(tensor.sqr(img2), axis=-1)\n",
    "    squaredPairwiseDistances = abs(xL2S - yL2S)    \n",
    "    threshold = tensor.sort(squaredPairwiseDistances)[:num_p][-1]\n",
    "\n",
    "    return threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For building a siamese network, instead of building the same network, I have used a symbolic variable where the first dimension is 2x batch size. In the input tensor, the odd elements of first dimension are image1 and the even dimension are image2. This trick is inspired from Sander Dilleman's comment here, https://github.com/Lasagne/Lasagne/issues/168#issuecomment-81134242. The `inp_var` variable's first dimension should always be an even number.\n",
    "The pairwise loss function is computed with the on-the-fly mined negative and positive values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating symbolic variables\n",
    "inp_var = tensor.tensor4()\n",
    "n_out = tensor.iscalar()\n",
    "num_p = tensor.iscalar()\n",
    "learning_rate = tensor.fscalar('learning_rate')\n",
    "# Batch size assumed to be 10 in this toy example\n",
    "batch_size = 10\n",
    "\n",
    "model_out = build_model(inp_var)['out']\n",
    "model_variable = ll.get_output(model_out)\n",
    "first_out, second_out = model_variable[0::2], model_variable[1::2]\n",
    "threshold = nearest_neighbour(first_out, second_out, num_p)\n",
    "\n",
    "# Pairwise Cor loss\n",
    "d = tensor.sum((first_out - second_out)**2, axis=1)\n",
    "loss = tensor.switch(d >= threshold, d, tensor.maximum(threshold - d, 0)).mean()\n",
    "l2_penalty = lasagne.regularization.regularize_network_params(model_out, lasagne.regularization.l2)\n",
    "loss = loss + l2_penalty\n",
    "\n",
    "# Get all the trainable params in the network\n",
    "network_params = ll.get_all_params(model_out, trainable=True)\n",
    "network_grad = tensor.grad(loss, network_params)\n",
    "updates = lasagne.updates.adam(network_grad, network_params, learning_rate=learning_rate)\n",
    "train_function = theano.function([inp_var, num_p, learning_rate], loss, updates=updates)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
